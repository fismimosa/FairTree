{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qf9bdVTW_Gk6"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import numba\n",
    "from numba import jit , njit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0lMmgOr9_Gk8"
   },
   "outputs": [],
   "source": [
    "# Global Config Variables\n",
    "n0 = 1000  # number of p=0 points in metric space\n",
    "V = n0  # Threshold for p=0\n",
    "K = 10# No of clusters\n",
    "A= 24# No of attributes\n",
    "iterations = 35  # maximum iteration in clustering\n",
    "runs =10\n",
    "option='Kmeans'  #Kmedian\n",
    "dataset ='CensusII'  #Bank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Mdaah1pG_Gk9"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "import pandas\n",
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from math import log2\n",
    "from sklearn import preprocessing\n",
    "import sys\n",
    "import timeit\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ulKWqyAF_Gk9"
   },
   "outputs": [],
   "source": [
    "import datetime\n",
    "import json\n",
    "import random\n",
    "from collections import defaultdict\n",
    "import matplotlib.patches as mpatches\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import cm\n",
    "#from pyclustering.cluster.kmedians import kmedians\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn import preprocessing\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(os.getcwd())\n",
    "def load_Dia(data_dir=''):\n",
    "\n",
    "    data_dir = data_dir\n",
    "    _path = 'diabetic_data_p.csv'\n",
    "    data_path = os.path.join(data_dir, _path)\n",
    "\n",
    "    K = 10\n",
    "\n",
    "    df = pandas.read_csv(data_path, sep=',')\n",
    "    print(df.head())\n",
    "    print(len(df))\n",
    "    \n",
    "    return df\n",
    "load_Dia()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kqd-lLX1_Gk-",
    "outputId": "8683c01f-923a-482b-ddca-32455f64d79f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ram/Downloads\n"
     ]
    }
   ],
   "source": [
    "print(os.getcwd())\n",
    "def load_CensusII(data_dir=''):\n",
    "\n",
    "    data_dir = data_dir\n",
    "    _path = 'USCensus1990_p.csv'\n",
    "    data_path = os.path.join(data_dir, _path)\n",
    "\n",
    "    K = 10\n",
    "\n",
    "    df = pandas.read_csv(data_path, sep=',')\n",
    "\n",
    "    \n",
    "    return df\n",
    "#load_CensusII()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hwYn0pJk_Gk_",
    "outputId": "b2aad18d-2e22-4a4f-c9e7-ac2282247386"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2458285\n",
      "2458285\n",
      "0.48472858110430644\n",
      "0.5152714188956935\n",
      "1191601\n",
      "1266684\n"
     ]
    }
   ],
   "source": [
    "if dataset=='CensusII':\n",
    "    df=load_CensusII()\n",
    "    df= df.round(decimals=5)\n",
    "    print(len(df))\n",
    "    df = df.dropna()\n",
    "    print(len(df))\n",
    "    #df['type'] = df['type']-1\n",
    "    typ = df['type'].values\n",
    "    #print(len(typ))\n",
    "    #print(df.head(10))\n",
    "    c1 = np.count_nonzero(typ == 0)\n",
    "    c2 = np.count_nonzero(typ == 1)\n",
    "\n",
    "    print(c1/(c1+c2))\n",
    "    print(c2/(c1+c2))\n",
    "\n",
    "    print(c1)\n",
    "    print(c2)\n",
    "\n",
    "elif dataset=='Diabetes':\n",
    "    df=load_Dia()\n",
    "    df= df.round(decimals=5)\n",
    "    print(len(df))\n",
    "    df = df.dropna()\n",
    "    #df = drop_duplicates()\n",
    "    print(len(df))\n",
    "    #df['type'] = df['type']-1\n",
    "    typ = df['type'].values\n",
    "    print(len(typ))\n",
    "    #print(df.head(10))\n",
    "    c1 = np.count_nonzero(typ == 0)\n",
    "    c2 = np.count_nonzero(typ == 1)\n",
    "\n",
    "    print(c1/(c1+c2))\n",
    "    print(c2/(c1+c2))\n",
    "\n",
    "    print(c1)\n",
    "    print(c2)\n",
    "\n",
    "    dfDropped = df.drop(columns=['type'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "otQqkDyY_Gk_"
   },
   "outputs": [],
   "source": [
    "def dual_print(f,*args,**kwargs):\n",
    "    #print(*args,**kwargs)\n",
    "    print(*args,**kwargs,file=f)\n",
    "\n",
    "def load_dataset(csv_name):\n",
    "    # read the dataset from csv_name and return as pandas dataframe\n",
    "    df = pd.read_csv(csv_name, header=None)\n",
    "    return df\n",
    "\n",
    "\n",
    "def k_random_index(df,K):\n",
    "    # return k random indexes in range of dataframe\n",
    "    return random.sample(range(0, len(df)), K)\n",
    "\n",
    "\n",
    "def find_k_initial_centroid(df,K):\n",
    "    centroids = []    # make of form [ [x1,y1]....]\n",
    "\n",
    "    rnd_idx = k_random_index(df,K)\n",
    "    #print(rnd_idx)\n",
    "    for i in rnd_idx:\n",
    "        coordinates =[]\n",
    "        for a in range(0,A):\n",
    "            coordinates.append(df.loc[i][a])\n",
    "        centroids.append(coordinates)   #df is X,Y,....., Type\n",
    "\n",
    "    return centroids\n",
    "\n",
    "#nOt using\n",
    "def calc_distance(x1, y1, x2, y2):\n",
    "    # returns the euclidean distance between two points\n",
    "    return ((x1 - x2) ** 2 + (y1 - y2) ** 2) ** 0.5\n",
    "\n",
    "def calc_distance_a(centroid, point):\n",
    "    #print('Ã§alculating distance\\n')\n",
    "\n",
    "    sum_ = 0\n",
    "\n",
    "    for i in range(0, len(centroid)):\n",
    "        sum_ = sum_ + (centroid[i]-point[i])**2\n",
    "\n",
    "    return sum_ #**0.5\n",
    "\n",
    "@njit(parallel=False)\n",
    "def find_distances_fast(k_centroids, df):\n",
    "    #print(\"Inside fast distances\")\n",
    "\n",
    "    dist = np.zeros((len(k_centroids),len(df),A+2),np.float64)\n",
    "    Kcnt = 0 \n",
    "    for c in k_centroids:  #K-centroid is of form [ c1=[x1,y1.....z1], c2=[x2,y2....z2].....]\n",
    "     \n",
    "        l = np.zeros((len(df),A+2),np.float64)\n",
    "        \n",
    "       \n",
    "        index = 0 \n",
    "        for row in df:                # row is now x,y,z......type\n",
    "            # append all coordinates to point\n",
    "            dis = np.sum((c- row[:A])**2)#calc_distance_a(c, point)\n",
    "            #Processing the vector for list\n",
    "            row_list = np.array([dis])\n",
    "            #append distance or l norm\n",
    "            row_list = np.append(row_list,row[:A+1])\n",
    "            #append all coordinates #append type of this row\n",
    "  \n",
    "            l[index] = row_list\n",
    "            index = index + 1\n",
    "            #l.append([calc_distance(c[0], c[1], row[0], row[1]), row[0], row[1], row[2]])  # [dist, X, Y,....Z , type]\n",
    "            # l contains list of type [dist,X,Y.....,Z,type] for each points in metric space\n",
    "        dist[Kcnt]= l\n",
    "        Kcnt = Kcnt + 1\n",
    "\n",
    "    # return dist which contains distances of all points from every centroid\n",
    "\n",
    "    return dist\n",
    "\n",
    "def find_distances(k_centroids, df):\n",
    "    dist = []\n",
    "    for c in k_centroids:  #K-centroid is of form [ c1=[x1,y1.....z1], c2=[x2,y2....z2].....]\n",
    "        l = []\n",
    "       \n",
    "\n",
    "        for index, row in df.iterrows():                # row is now x,y,z......type\n",
    "            point =[]\n",
    "            for a in range(0, A):\n",
    "                point.append(row.iloc[a])  # append all coordinates\n",
    "\n",
    "            dis = calc_distance_a(c, point)\n",
    "            #Processing the vector for list\n",
    "            row_list = [dis]\n",
    "            #append distance or l norm\n",
    "\n",
    "            for a in range(0, A):\n",
    "                row_list.append(row.iloc[a])   #append all coordinates\n",
    "            \n",
    "            row_list.append(row.iloc[a+1])   #append type of this row\n",
    "\n",
    "            l.append(row_list)\n",
    "            #l.append([calc_distance(c[0], c[1], row[0], row[1]), row[0], row[1], row[2]])  # [dist, X, Y,....Z , type]\n",
    "            # l contains list of type [dist,X,Y.....,Z,type] for each points in metric space\n",
    "        dist.append(l)\n",
    "\n",
    "    # return dist which contains distances of all points from every centroid\n",
    "\n",
    "    return dist\n",
    "\n",
    "\n",
    "def sort_and_valuation(dist):\n",
    "    sorted_val = []\n",
    "   \n",
    "    for each_centroid_list in dist:\n",
    "        each_centroid_list_sorted = sorted(each_centroid_list, key=lambda x: (x[A+1], x[0]))  # A+1 is index of type , 0 is dist\n",
    "        sorted_val.append(each_centroid_list_sorted)\n",
    "\n",
    "        # sort on basis of type & then dist.\n",
    "        # Now all whites are towards start and all black are after white as they have additional V added to their valuation\n",
    "        # Among the whites, the most closest is at start of list as it has more valuation.\n",
    "        # Similarly sort the black points among them based on distance as did with white\n",
    "\n",
    "    return sorted_val\n",
    "\n",
    "\n",
    "def clustering(sorted_valuation, hashmap_points,K):\n",
    "    n = len(sorted_valuation[0])  # total number of points in metric space\n",
    "    \n",
    "\n",
    "    cluster_assign = []\n",
    "\n",
    "    for i in range(0, K):\n",
    "        cluster_assign.append([])  # initially all clusters are empty\n",
    "    \n",
    "    map_index_cluster = []\n",
    "    for i in range(0,K+2):\n",
    "        map_index_cluster.append(0)\n",
    "        #initially check all sorted evaluation from 0th index \n",
    "    \n",
    "    number_of_point_alloc = 0\n",
    "    curr_cluster = 0\n",
    "   \n",
    "    # until all points are allocated\n",
    "    while number_of_point_alloc != n:  # As convergence is guaranteed that all points will be allocated to some cluster set\n",
    "        #print('Number of point alloc : '+str(number_of_point_alloc))\n",
    "        start_inde = map_index_cluster[curr_cluster % K]\n",
    "        \n",
    "        for inde in range(start_inde,len(sorted_valuation[curr_cluster % K])):\n",
    "            each = sorted_valuation[curr_cluster % K][inde]\n",
    "            # each is (dist,X,Y,....Z,type)\n",
    "            \n",
    "            if hashmap_points[tuple(each[1: ])] > 0:    # each is (dist, X,Y,....Z, type)\n",
    "                cluster_assign[curr_cluster].append(each)\n",
    "                hashmap_points[tuple(each[1: ])] -= 1\n",
    "                number_of_point_alloc += 1\n",
    "                map_index_cluster[curr_cluster % K] = inde  #next time start from here as isse prev all allocated\n",
    "                break\n",
    "\n",
    "        curr_cluster = (curr_cluster + 1) % K\n",
    "   \n",
    "    return cluster_assign\n",
    "\n",
    "\n",
    "def update_centroids_median(cluster_assign,K):\n",
    "    new_centroids = []\n",
    "    for k in range(0, K):\n",
    "        #print(k)\n",
    "        cAk =  np.array(cluster_assign[k])\n",
    "        cAk = np.delete(cAk,[0,-1],axis=1)\n",
    "        #print(len(cAk))\n",
    "        if len(cAk) %2 ==0 and len(cAk)>0: \n",
    "            cc = [np.median(np.array(cAk[:-1])[:,cl]) for cl in range(0,cAk.shape[1])]\n",
    "            new_centroids.append(cc)\n",
    "        elif len(cAk) %2 !=0 and len(cAk)>0:\n",
    "            cc = [np.median(np.array(cAk)[:,cl]) for cl in range(0,cAk.shape[1])]\n",
    "            new_centroids.append(cc)\n",
    "        elif len(cAk)==0:\n",
    "            print(\"Error: No centroid found updation error\")\n",
    "    \n",
    "    return new_centroids\n",
    "        \n",
    "\n",
    "\n",
    "def update_centroids(cluster_assign,K):\n",
    "\n",
    "    new_centroids = []\n",
    "    for k in range(0, K):\n",
    "\n",
    "        sum_a = []\n",
    "\n",
    "        for i in range(0, A):\n",
    "            sum_a.append(0)\n",
    "\n",
    "        for each in cluster_assign[k]:\n",
    "            sum_a = [sum(x) for x in zip(sum_a, each[1:-1])]\n",
    "            #each is (dist,X,Y,.....Z,type)\n",
    "       # print('sum a is '+str(sum_a))\n",
    "        new_coordinates = []\n",
    "        for a in range(0, A):\n",
    "            new_coordinates.append(sum_a[a] / len(cluster_assign[k]))\n",
    "        new_centroids.append(new_coordinates)\n",
    "        k=k+1\n",
    "\n",
    "\n",
    "\n",
    "    return new_centroids\n",
    "\n",
    "\n",
    "def calc_clustering_objective(k_centroid, cluster_assign,K):\n",
    "    cost = 0\n",
    "\n",
    "    for k in range(0, K):\n",
    "\n",
    "        for each in cluster_assign[k]:  #each is (dist, X,Y,....,Z,type)\n",
    "            dd = calc_distance_a(k_centroid[k], each[1:-1])\n",
    "            cost = cost + (dd)\n",
    "\n",
    "    return cost\n",
    "\n",
    "\n",
    "def calc_fairness_error(df, cluster_assign,K):\n",
    "    U = []  # distribution of each type in original target dataset for each J = 0 , 1....\n",
    "    P_k_sum_over_j = []  # distribution in kth cluster  sum_k( sum_j(   Uj * j wale/total_in_cluster ) )\n",
    "\n",
    "    f_error = 0\n",
    "    cnt_j_0 = 0\n",
    "    cnt_j_1 = 0\n",
    "  #  cnt_j_2 = 0  \n",
    "    cnt = 0\n",
    "    for index, row in df.iterrows():\n",
    "        if row.iloc[-1] == 1:\n",
    "            cnt_j_1 += 1\n",
    "        elif row.iloc[-1] == 0:\n",
    "            cnt_j_0 += 1\n",
    "      #  elif row.iloc[-1] == 2:\n",
    "        #    cnt_j_2 += 1\n",
    "            \n",
    "        cnt += 1\n",
    "\n",
    "    U.append(cnt_j_0 / cnt)\n",
    "    U.append(cnt_j_1 / cnt)\n",
    "    #U.append(cnt_j_2 / cnt)\n",
    "\n",
    "   \n",
    "\n",
    "    for k in range(0, K):  # for each cluster\n",
    "\n",
    "        for j in range(0, len(U)):   #for each demographic group\n",
    "\n",
    "            cnt_j_cluster = 0\n",
    "            cnt_total = 0\n",
    "\n",
    "            for each in cluster_assign[k]:\n",
    "                if int(each[-1]) == j:    #each is (dist,X, Y.....,Z,type)\n",
    "                    cnt_j_cluster += 1\n",
    "                cnt_total += 1\n",
    "                \n",
    "            if cnt_j_cluster !=0 and cnt_total != 0:\n",
    "                P_k_sum_over_j.append(-U[j] * np.log((cnt_j_cluster / cnt_total)/U[j]))\n",
    "            else:\n",
    "                P_k_sum_over_j.append(0)  #log(0)=0 considered\n",
    "\n",
    "    for each in P_k_sum_over_j:\n",
    "        f_error += each\n",
    "\n",
    "    return f_error\n",
    "\n",
    "\n",
    "def calc_balance(cluster_assign,K):\n",
    "    S_k = []  # balance of each k cluster\n",
    "    balance = 0  # min (S_k)\n",
    "\n",
    "    for k in range(0, K):\n",
    "        cnt_j_0 = 0\n",
    "        cnt_j_1 = 0\n",
    "       # cnt_j_2 = 0\n",
    "        cnt = 0\n",
    "        for each in cluster_assign[k]:\n",
    "\n",
    "            if int(each[-1]) == 1:\n",
    "                cnt_j_1 += 1\n",
    "            elif int(each[-1]) == 0:\n",
    "                cnt_j_0 += 1\n",
    "           # elif int(each[-1]) == 2:\n",
    "           #     cnt_j_2 += 1\n",
    "                \n",
    "            cnt += 1\n",
    "\n",
    "        if cnt_j_0 != 0 and cnt_j_1 != 0 :#and cnt_j_2!= 0:\n",
    "            S_k.append(min([cnt_j_0 / cnt_j_1, cnt_j_1 / cnt_j_0 ]))#, cnt_j_1 / cnt_j_2 , cnt_j_2 / cnt_j_1 , cnt_j_0 / cnt_j_2, cnt_j_2 / cnt_j_0 ]))\n",
    "        elif cnt_j_0 == 0 or cnt_j_1 ==0  :#or cnt_j_2==0:\n",
    "            S_k.append(0)\n",
    "\n",
    "\n",
    "\n",
    "    balance = min(S_k)\n",
    "\n",
    "    return balance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_F8SOnt8_GlB"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def main():\n",
    "    # Step1 : Load the dataset\n",
    "            \n",
    "    list_fair_K=[]\n",
    "    list_obj_K =[]       \n",
    "    list_balance_K=[]\n",
    "    \n",
    "  #  os.makedirs('kmeansCensusII')\n",
    "    \n",
    "    for kk in [2,5,10,15,20,30,40]:\n",
    "        K = kk\n",
    "        \n",
    "        print(\" K==\"+str(K)+\"  \")\n",
    "        \n",
    "        list_fair_run=[]\n",
    "        list_obj_run =[]       \n",
    "        list_balance_run=[]\n",
    "        seeds = [0,100,200,300,400,500,600,700,800,900,1000,1100]\n",
    "      \n",
    "        for run in range(0,runs):\n",
    "            np.random.seed(seeds[run])\n",
    "            random.seed(seeds[run])\n",
    "            f = open('kmeansCensusII/K_'+str(K)+'_run_'+str(run)+'_output.txt', 'a')\n",
    "            print(\"+\"*100)\n",
    "            print('                        RUN  : '+ str(run))\n",
    "\n",
    "\n",
    "            list_fair_iter=[]\n",
    "            list_obj_iter =[]\n",
    "            list_balance_iter=[]\n",
    "\n",
    "            \n",
    "            \n",
    "            # Step2 : Find initial K random centroids using k_random_index(df) & find_k_initial_centroid(df)\n",
    "            k_centroid = find_k_initial_centroid(df,kk)\n",
    "            \n",
    "\n",
    "            prev_assignment =[]\n",
    "            cluster_assignment = []\n",
    "\n",
    "            for i in range(0, K):\n",
    "                cluster_assignment.append([])  # initially all clusters are empty\n",
    "\n",
    "            sum_time = 0\n",
    "            curr_itr = 0\n",
    "            prev_objective_cost=-1\n",
    "            objective_cost = 0\n",
    "                # Step3 : Find distances from the centroids using find_distances() with list of [ [x1,y1,z1..] , [x2,y2,z2..]....] centroids format list\n",
    "            while curr_itr <= iterations:# and prev_objective_cost != objective_cost:\n",
    "\n",
    "                start = time.process_time()#timeit.default_timer()\n",
    "               \n",
    "\n",
    "                dual_print(f,'Calulating distance for iteration : '+ str(curr_itr)+'\\n')\n",
    "                df1 = df.values\n",
    "                k_centroids1= np.array(k_centroid)\n",
    "                \n",
    "\n",
    "                dist = find_distances_fast(k_centroids1, df1)\n",
    "               \n",
    "                dual_print(f,'Finished calc distance for iteration : '+ str(curr_itr)+'\\n')\n",
    "                # Step4 :  Find Valuation matrix for all centroids using sort_and_valuation()\n",
    "             \n",
    "                dual_print(f,'Calulating Valuation for iteration : '+ str(curr_itr)+'\\n')\n",
    "\n",
    "                valuation = sort_and_valuation(dist)\n",
    "               \n",
    "                dual_print(f,'Finished Valuation for iteration : '+ str(curr_itr)+'\\n')\n",
    "\n",
    "                #Step5 : Perform clustering using valuation matrix & hashmap of all points in metric\n",
    "                hash_map = {}\n",
    "                for index, row in df.iterrows():\n",
    "                    temp = tuple(row[:-1])\n",
    "                    if tuple(row) in hash_map.keys():\n",
    "                        hash_map[tuple(row)] = int(hash_map[tuple(row)]) + 1\n",
    "                    else:\n",
    "                        hash_map.update({tuple(row): int(1)})   #dict is of form { (x,y): 0 , ....}\n",
    "\n",
    "             \n",
    "                dual_print(f,'Finding clusters for iteration : '+ str(curr_itr)+'\\n')\n",
    "                prev_assignment = cluster_assignment\n",
    "                cluster_assignment = clustering(valuation, hash_map,K)\n",
    "\n",
    "                dual_print(f,'Finished finding cluster for iteration : '+ str(curr_itr)+'\\n')\n",
    "\n",
    "                \n",
    "                balance = calc_balance(cluster_assignment,K)\n",
    "              \n",
    "                f_error = calc_fairness_error(df, cluster_assignment,K)\n",
    "              \n",
    "                clustering_cost = calc_clustering_objective(k_centroid,cluster_assignment,K)\n",
    "                if curr_itr!=0:\n",
    "                     prev_objective_cost = objective_cost\n",
    "                    \n",
    "                objective_cost = np.round(clustering_cost,3)\n",
    "\n",
    "\n",
    "                \n",
    "\n",
    "                list_balance_iter.append(str(balance))\n",
    "                list_obj_iter.append(str(objective_cost))\n",
    "                list_fair_iter.append(str(f_error))\n",
    "\n",
    "                dual_print(f,'balance : ' + str(balance) + '\\n')\n",
    "                dual_print(f,'Fairness Error : ' + str(f_error) + '\\n')\n",
    "                dual_print(f,'Clustering Objective/Cost ' + str(clustering_cost) + '\\n')\n",
    "\n",
    "                #Step6 : Print the cluster assignments\n",
    "                if abs(objective_cost-prev_objective_cost) <= 30000:\n",
    "                    break;\n",
    "                \n",
    "                dual_print(f,'Updating centroids for iteration : '+ str(curr_itr)+'\\n')\n",
    "\n",
    "                #Step7 : Find new centroids using mean of all points in current assignment\n",
    "                if option=='Kmeans':\n",
    "                    k_centroid = update_centroids(cluster_assignment,K)\n",
    "                else:\n",
    "                     k_centroid = update_centroids_median(cluster_assignment,K)\n",
    "                        \n",
    "                dual_print(f,'Finished centroid updation for iteration : '+ str(curr_itr)+'\\n')\n",
    "                dual_print(f,'Iteration No: '+str(curr_itr)+' : updated centroid are : '+ str(k_centroid))\n",
    "                #Step8 : Repeat from Step3 until clusters are same or iterations reach upper limit\n",
    "                stop = time.process_time()#timeit.default_timer()\n",
    "                sum_time += (stop - start)\n",
    "                dual_print(f,'Time for iteration : ' + str(curr_itr) + ' is  ' + str(stop - start) + '\\n')\n",
    "\n",
    "                curr_itr += 1\n",
    "\n",
    "                dual_print(f,'-----------------------------Finished-----------------------------------------------\\n')\n",
    "\n",
    "\n",
    "                \n",
    "\n",
    "            print('Total time taken to converge '+ str(sum_time)+'\\n')\n",
    "            print('Iterations total taken for convergence : '+str(curr_itr-1)+'\\n')\n",
    "\n",
    "            dual_print(f,'Total time taken is '+ str(sum_time)+'\\n')\n",
    "            dual_print(f,'Iterations total : '+str(curr_itr-1))\n",
    "           \n",
    "            #Step 10 : Find balance , fairness error , and clustering objective or cost\n",
    "\n",
    "            balance_converged = calc_balance(cluster_assignment,K)\n",
    "            f_error_converged = calc_fairness_error(df, cluster_assignment,K)\n",
    "            clustering_cost_converged = calc_clustering_objective(k_centroid,cluster_assignment,K)\n",
    "\n",
    "            print(\"\\nCost variation over iterations\")\n",
    "            print(list_obj_iter)\n",
    "            print(\"\\nBalance variation over iterations\")\n",
    "            print(list_balance_iter)\n",
    "            print(\"\\nFairness error over iterations\")\n",
    "            print(list_fair_iter)\n",
    "            print('\\n')\n",
    "            \n",
    "            print('Final converged balance : ' + str(balance_converged) + '\\n')\n",
    "            print('Final Converged Fairness Error : ' + str(f_error_converged) + '\\n')\n",
    "            print('Final converged Clustering Objective/Cost ' + str(clustering_cost_converged) + '\\n')\n",
    "\n",
    "            dual_print(f,'Converged balance : ' + str(balance_converged) + '\\n')\n",
    "            dual_print(f,'Converged Fairness Error : ' + str(f_error_converged) + '\\n')\n",
    "\n",
    "            dual_print(f,'Converged Clustering Objective/Cost ' + str(clustering_cost_converged) + '\\n')\n",
    "            \n",
    "            f.close()\n",
    "            run  = run +1\n",
    "            list_obj_run.append(clustering_cost_converged)\n",
    "            list_fair_run.append(f_error_converged)\n",
    "            list_balance_run.append(balance_converged)\n",
    "        \n",
    "        print(\"@\"*70)\n",
    "        print(\"Cost variations over run\")\n",
    "        print(str(list_obj_run))\n",
    "        print(\"balance variations over run\")\n",
    "        print(str(list_balance_run))\n",
    "        print(\"fairness error over run\")\n",
    "        print(str(list_fair_run))\n",
    "        print(\"#\"*30)\n",
    "        print(\"Mean Cost variations over run\")\n",
    "        print(str(np.mean(np.array(list_obj_run))))\n",
    "        print(\"Std Dev Cost variations over run\")\n",
    "        print(str(np.std(np.array(list_obj_run))))\n",
    "        print(\"#\"*30)\n",
    "        \n",
    "        list_obj_K.append(np.mean(np.array(list_obj_run)))\n",
    "        list_fair_K.append(np.mean(np.array(list_fair_run)))\n",
    "        list_balance_K.append(np.mean(np.array(list_balance_run)))\n",
    "        \n",
    "    print(\"%\"*70)\n",
    "    print(\"Cost variations over K\")\n",
    "    print(str(list_obj_K))\n",
    "    print(\"balance variations over K\")\n",
    "    print(str(list_balance_K))\n",
    "    print(\"fairness error over K\")\n",
    "    print(str(list_fair_K))\n",
    "    print(\"#\"*30)\n",
    "\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mU0MC-Yi_GlC",
    "outputId": "35e633ad-9e86-4ee7-b4b2-2d917b3aafe7"
   },
   "outputs": [],
   "source": [
    "\n",
    "import time\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "EF1__equal_Adult-fast-Kmeans_f-CensusII_30_new.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
